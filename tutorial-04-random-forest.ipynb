{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "### What is a random forest?\n",
    "A random forest is an improved version of decision trees. A decision tree is a single DAG which takes in inputs and produces outputs. A random forest is a collection of decision trees (a forest). Each decision tree is generated at the same time, with slightly different (random) input parameters (different numbers of nodes, different graph shape, different starting values). Random forest is a \"bagging\" algorithm, which means every decision tree is given the same input, they all produce a differnet output, and then their outputs are aggregated to produce the model's decision. For categorical models, the majority decision wins. For quantitative models, the outputs are averaged.\n",
    "\n",
    "![A visual representation of a random forest](assets/random_forest.png)\n",
    "\n",
    "### Why is random forest useful?\n",
    "ML models are inhrently flawed. They get things wrong. One way they get things wrong is \"overfitting\". When a model overfits, it gets very good at predicting the data in its training set, but does so via quirks in the training data rather than the looking at the actual data. For example, if a model is training apples and oranges, and all apples in the training set have sticker labels on them while all the oranges in the training set don't, the model might overfit to detect the sticker label. On the training set, it would detect nearly 100% correctly, but given a test set with apples that don't have stickers or oranges that do have stickers, it would get all of the predictions wrong.\n",
    "\n",
    "### How to use random forest?\n",
    "Random forest is used in a very similar way to decision trees. It will be trained in the same way, and inference works the same way; it is a black box which takes in input and gives an ouput. The different is in initialization. There are more settings to control with random forest: the number of trees, the type of randomization, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, import libraries\n",
    "# Remember, if any of these fail to import, they need to be\n",
    "# installed in the terminal via 'pip install PACKAGENAME'\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# XXX Import the load_digits dataset\n",
    "from XXX import XXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, load the dataset\n",
    "#XXX load the digits dataset with load_digits()\n",
    "digits = XXX\n",
    "\n",
    "x = digits.data\n",
    "y = digits.target\n",
    "\n",
    "# Pandas is a library that makes it easy to display data in\n",
    "# a readable format. To use it, create a DataFrame. The DataFrame\n",
    "# can be fed data (numbers) and columns (labels).\n",
    "data = pd.DataFrame(data=digits.data, columns=digits.feature_names)\n",
    "display(data)\n",
    "display(x)\n",
    "display(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, split the data into train and test data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n",
    "\n",
    "print(np.size(x_train))\n",
    "print(np.size(x_test))\n",
    "print(np.size(y_train))\n",
    "print(np.size(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, clean up the data. StandardScalar removes the mean and scales to unit variance.\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, initialize the model.\n",
    "# There are many parameters which you could pass to Random ForestClassifier.\n",
    "# You should try messing around with different parameters. Here are some options:\n",
    "# random_state determines the random initalization in each of the trees.\n",
    "# n_jobs determines how many processes run in parallel. Default is 1. -1 means use all processors.\n",
    "# max_depth determines how many layers of logic an individual decision tree can have\n",
    "# n_estimators determines how many decision trees are in the forst\n",
    "# oob_score means the model is validated on a unique set of datapoints, rather than sometimes reusing data in the same training / testing run.\n",
    "# oob stands for out of bag, and is a little complicated. You can read about it at https://towardsdatascience.com/what-is-out-of-bag-oob-score-in-random-forest-a7fa23d710.\n",
    "# You can read about the other options at https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "model = RandomForestClassifier(random_state=2,\n",
    "                               n_jobs=-1,\n",
    "                               min_samples_leaf=100,\n",
    "                               max_depth=5,\n",
    "                               n_estimators=50,)\n",
    "model = RandomForestClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, train the model.\n",
    "\n",
    "# Jupyter Notebook has \"magic\" statements which do microfunctions.\n",
    "# if you start a line with %time, it will measure the time to execute\n",
    "# that line. You can do the same thing for a whole cell by putting\n",
    "# %%time at the start of the cell.\n",
    "%time model = model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Model\n",
    "\n",
    "model.predict(x_test)\n",
    "print(f\"The model has an accuracy of {model.score(x_test, y_test)*100:.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This accuracy score is really bad. Lets try to make it beter.\n",
    "# Hyperparametarization is the process of figuring out better\n",
    "# ML model parameters by running a lot of models and seeing which\n",
    "# performs best.\n",
    "\n",
    "hyperparam_model = RandomForestClassifier(n_jobs=-1)\n",
    "params = {\n",
    "    'random_state':[2,40,152,9836],\n",
    "    'min_samples_leaf':[20,50,100],\n",
    "    'max_depth':[3,5,10,20],\n",
    "    'n_estimators':[50,100,500],\n",
    "}\n",
    "\n",
    "# The grid search will try every permutation of the parameters and find the best.\n",
    "# This took 1 minute on my 8 core machine. Yours may take longer.\n",
    "# This will throw an error that says \"The least populated class has only 1 members\".\n",
    "# That is because n_jobs is not being parametrized.\n",
    "grid_search = GridSearchCV(estimator=hyperparam_model,\n",
    "                           param_grid=params,\n",
    "                           n_jobs=-1,\n",
    "                           verbose=5,\n",
    "                           cv=4,\n",
    "                           scoring=\"accuracy\")\n",
    "\n",
    "%time grid_search.fit(x_train, y_train)\n",
    "print(f\"The best model accuracy is {grid_search.best_score_}.\")\n",
    "print(f\"The best model is {grid_search.best_estimator_}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
