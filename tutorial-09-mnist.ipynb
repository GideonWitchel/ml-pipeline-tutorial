{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying the MNIST Database\n",
    "### The MNIST Database\n",
    "The MNIST database stands for the Modified National Institute of Standards and Technology database. This is a set of 60,000 labeled training images and 10,000 testing images, each consisting of a hand drawn number from 0 to 9 stored in a 28 by 28 pixel grid. The MNIST is a standard for training and testing machine learning algorithms. Below are examples of the digits from the training set.\n",
    "\n",
    "![Examples of hand drawn digits from the MNIST database](assets/mnist-examples.png)\n",
    "\n",
    "### Classifying the MNIST Databse\n",
    "As a final project for this set of tutorials, we will be performing classification on the MNIST database using various machine learning algorithms. The best model in the world, developed by the University of Virginia, has an 0.18% error rate. 0% is functionally impossible, because somme of the digits are basically unreadable, but try to get as accurate as you can.\n",
    "\n",
    "If you're interested in seeing a more complex approach, you can read about a tensorflow model with a 2% error rate here: https://www.kaggle.com/code/mohammedsalahuddin/mnist-dataset-with-98-03-accuracy\n",
    "\n",
    "\n",
    "Refer to the explainations below and have fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#Classifier Imports\n",
    "# Remember, if any are missing, install with pip\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# XXX Import RandomForestClassifier, GradientBoostingClassifier, and XGBClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mnist dataset (This will take around 30 seconds)\n",
    "mnist = fetch_openml(\"mnist_784\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some of the digits\n",
    "image= mnist.data.to_numpy()\n",
    "plt.imshow((image[7].reshape(28,28)), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "\n",
    "# What is the number in the 'image' array at location 1?\n",
    "# What about at location 2?\n",
    "# What about at location 580?\n",
    "# What about at location 100?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "# Randomize the order of the data and scrub the old indecies\n",
    "index_number= np.random.permutation(70000)\n",
    "x1 = mnist.data.loc[index_number]\n",
    "y1 = mnist.target.loc[index_number]\n",
    "x1.reset_index(drop=True,inplace=True)\n",
    "y1.reset_index(drop=True,inplace=True)\n",
    "\n",
    "# Split the data into training and testing\n",
    "# The commented out version is the full dataset. The other is a\n",
    "# smaller subset of 10% of the data, which is useful for a speedier\n",
    "# tutorial. Try expanding and contracting the size of the training\n",
    "# and testing sets to see how it affects accuracy and training time.\n",
    "\n",
    "#x_train , x_test = x1[:55000], x1[55000:]\n",
    "#y_train , y_test = y1[:55000], y1[55000:]\n",
    "\n",
    "x_train , x_test = x1[:5500], x1[5500:7000]\n",
    "y_train , y_test = y1[:5500], y1[5500:7000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Classifier\n",
    "dtc=DecisionTreeClassifier()\n",
    "dtc.fit(x_train,y_train)\n",
    "dtc_result = dtc.predict(x_test)\n",
    "print('Accuracy :',accuracy_score(y_test,dtc_result))\n",
    "print(classification_report(y_test,dtc_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "rfc=RandomForestClassifier()\n",
    "# XXX Fit and preict using x_train, y_train, and x_test\n",
    "\n",
    "rfc_result = \n",
    "\n",
    "print('Accuracy :',accuracy_score(y_test,rfc_result))\n",
    "print(classification_report(y_test,rfc_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosted Classifier (this one takes especially long to execute)\n",
    "# Declare, fit, and train the classifier\n",
    "\n",
    "# Find the accuracy and classification report with the same commands as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Classifier\n",
    "# XGBoost requires a little bit of extra data preprocessing\n",
    "le = LabelEncoder()\n",
    "y_train_xgb = le.fit_transform(y_train)\n",
    "y_test_xgb = le.fit_transform(y_test)\n",
    "\n",
    "# Declare, fit, and train the classifier with x_train, y_train_xgb, and x_test\n",
    "\n",
    "# Find the accuracy and classification report with the same commands as above and y_test_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "# Feel free to tune any models you want! Try tuning at least 1 model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
