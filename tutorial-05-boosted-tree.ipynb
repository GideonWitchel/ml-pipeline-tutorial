{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting\n",
    "### What is gradient boosting?\n",
    "Gradient boosting, like random forest, is a strategy for improving the accuracy of decision trees. While random forest is a \"bagging\" strategy, training trees in parallel and then picking the majority-rules decision, gradient boosting is a \"boosting\" strategy. Boosting strategies train trees in series, each improving off the previous tree. The final iteration is the model.\n",
    "\n",
    "### How does gradient boosting work?\n",
    "Gradient boosting works by fixing the worst part of the previous decision tree. If there is a specific node that is causing the most problems in the decisionmaking process, that node will get randomized. This process repeats, slowly specifying the model to get better and better.\n",
    "\n",
    "### How is gradient boosting used?\n",
    "Gradient boosting can be used in any situation where a decision tree or random frest can be used. It can solve regression or classification problems.\n",
    "\n",
    "### Boosted Forests\n",
    "Gradient boosting can also be used on enitre forests of decision trees. This is a strategy which can recieve the benifits of both gradient boosting and random forest algorithms, but at the cost of increased computation.\n",
    "\n",
    "![Visual depictions of the four types of learning trees](assets/learning_trees.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Exercise\n",
    "Gradient boosting is implemented in a similar way to random forest, insofar as it is a decision trees with extra parameters for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# XXX Import the load_diabetes dataset from sklearn.datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and split the dataset\n",
    "# XXX Set the test_size to 20% of the dataset\n",
    "# XXX Set the random_state to 2 for reproducale results\n",
    "x, y = load_diabetes(return_X_y=True)\n",
    "train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=XXX, random_state=XXX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the model\n",
    "model = GradientBoostingRegressor(learning_rate=0.1,\n",
    "                                  n_estimators=300,\n",
    "                                  max_depth=1,\n",
    "                                  random_state=2,\n",
    "                                  max_features=5)\n",
    "\n",
    "# XXX train the model with the fit command\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX Test the model with the test_x set\n",
    "model.predict(XXX)\n",
    "print(f\"The model has an accuracy of {model.score(test_x, test_y)*100:.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets do some Hyperparametarization!\n",
    "# Change the parmas to see if you can discover a better model.\n",
    "\n",
    "hyperparam_model = GradientBoostingRegressor()\n",
    "params = {\n",
    "    'learning_rate':[0.05,0.1,0.15,0.2,0.25],\n",
    "    'max_depth':[1,3,5,10,20],\n",
    "    'n_estimators':[1,50,100,300,500],\n",
    "}\n",
    "\n",
    "# This took 2 minutes on my 8 core machine. Yours may take longer.\n",
    "# This takes longer than random forest or decision tree individually,\n",
    "# but can be more accurate.\n",
    "grid_search = GridSearchCV(estimator=hyperparam_model,\n",
    "                           param_grid=params,\n",
    "                           verbose=5,\n",
    "                           cv=5,)\n",
    "\n",
    "%time grid_search.fit(train_x, train_y)\n",
    "print(f\"The best model accuracy is {grid_search.best_score_}.\")\n",
    "print(f\"The best model is {grid_search.best_estimator_}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
